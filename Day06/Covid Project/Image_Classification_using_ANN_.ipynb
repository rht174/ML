{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JT4a4dwlqhsd"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwMdAK1XqmPI"
   },
   "source": [
    "#Project 1: Covid 19 Prediction using Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9dbM1bxjvnU"
   },
   "source": [
    "Dataset: [Covid 19 Chest X-ray dataset](https://www.kaggle.com/tawsifurrahman/covid19-radiography-database)\n",
    "\n",
    "\n",
    "A team of researchers from Qatar University, Doha, Qatar, and the University of Dhaka, Bangladesh along with their collaborators from Pakistan and Malaysia in collaboration with medical doctors have created a database of chest X-ray images for COVID-19 positive cases along with Normal and Viral Pneumonia images. This COVID-19, normal, and other lung infection dataset is released in stages. In the first release, we have released 219 COVID-19, 1341 normal, and 1345 viral pneumonia chest X-ray (CXR) images. In the first update, we have increased the COVID-19 class to 1200 CXR images. In the 2nd update, we have increased the database to 3616 COVID-19 positive cases along with 10,192 Normal, 6012 Lung Opacity (Non-COVID lung infection), and 1345 Viral Pneumonia images. We will continue to update this database as soon as we have new x-ray images for COVID-19 pneumonia patients.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mHnC6nNtBK1"
   },
   "source": [
    "**1. Mount the Google Drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gORvsC4_s_fY",
    "outputId": "ca6dca8c-75f7-4a8a-f45c-82819976d85b"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiZOocimtGeu"
   },
   "source": [
    "**2. Move to the place where data resides**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EdbGFIkPlah9",
    "outputId": "66024259-89ad-4fc5-d820-94a56ca7b5be"
   },
   "outputs": [],
   "source": [
    "# %cd /content/drive/MyDrive/24Aug21/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lqViJFpyjxJZ",
    "outputId": "ef53b5e0-93ec-4792-9f68-66aca0bbf68c"
   },
   "outputs": [],
   "source": [
    "# !ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0e8v2RY-tKUs"
   },
   "source": [
    "**3. Unziping the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PYfX_i50jzOc",
    "outputId": "8c6013f7-6d59-489f-ccd0-9bdf2e1f0b05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unzip in c:\\users\\rohit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kOWDshT6lb8K",
    "outputId": "a2de7b64-6442-40de-8adf-7824bf094abb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!unzip archive.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvuRmzA0tNw0"
   },
   "source": [
    "**4. Install split folder python package**\n",
    "\n",
    "https://pypi.org/project/split-folders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r-4b_r2qlenB",
    "outputId": "33ae9e98-d78b-492f-a0f1-0cd3d9c24285"
   },
   "outputs": [],
   "source": [
    "# !pip install split_folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yq0KuuUatVi-"
   },
   "source": [
    "**5. Splitting the data in training, testing and validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8xXGHVOIlheA",
    "outputId": "8e371c16-00c3-4632-c16b-809f59de98c6"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The provided input folder \"covid_dataset\" does not exists. Your relative path cannot be found from the current working directory \"C:\\Users\\rohit\\OneDrive\\synced_work\\Summer Industrial Training Machine Learning\\Machine-Learning-Training\\Day6\\Covid Project\".",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msplitfolders\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43msplitfolders\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mratio\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcovid_dataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msplit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1337\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\splitfolders\\split.py:81\u001b[0m, in \u001b[0;36mratio\u001b[1;34m(input, output, seed, ratio, group_prefix, move)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ratio) \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`ratio` should\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 81\u001b[0m \u001b[43mcheck_input_format\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_tqdm:\n\u001b[0;32m     84\u001b[0m     prog_bar \u001b[38;5;241m=\u001b[39m tqdm(desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCopying files\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m files\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\splitfolders\\split.py:56\u001b[0m, in \u001b[0;36mcheck_input_format\u001b[1;34m(input)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m p_input\u001b[38;5;241m.\u001b[39mis_absolute():\n\u001b[0;32m     55\u001b[0m         err_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Your relative path cannot be found from the current working directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPath\u001b[38;5;241m.\u001b[39mcwd()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err_msg)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m p_input\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe provided input folder \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not a directory\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: The provided input folder \"covid_dataset\" does not exists. Your relative path cannot be found from the current working directory \"C:\\Users\\rohit\\OneDrive\\synced_work\\Summer Industrial Training Machine Learning\\Machine-Learning-Training\\Day6\\Covid Project\"."
     ]
    }
   ],
   "source": [
    "import splitfolders\n",
    "splitfolders.ratio(\"covid_dataset\", output=\"split\", seed=1337, ratio=(.8, .1, .1), group_prefix=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_rTBEKCtaTR"
   },
   "source": [
    "**6. Loading the dataset with normalization in batches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4of-TiQPlkCZ",
    "outputId": "03db4a80-b769-4c98-b2c0-aef647af99ec"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Normalize training and validation data in the range of 0 to 1\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Read the training sample and set the batch size \n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'split/train/',\n",
    "        target_size=(128, 128),\n",
    "        batch_size=8,\n",
    "        seed=100,\n",
    "        class_mode='categorical')\n",
    "\n",
    "# Read Validation data from directory and define target size with batch size\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        'split/val/',\n",
    "        target_size=(128, 128),\n",
    "        batch_size=8,\n",
    "        class_mode='categorical',\n",
    "        seed=1000,\n",
    "        shuffle=False)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        'split/test/',\n",
    "        target_size=(128, 128),\n",
    "        batch_size=8,\n",
    "        seed=500,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqmlFmyitf8O"
   },
   "source": [
    "**7. Model Building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RrA7f0eKl4tH",
    "outputId": "2790cf4f-f6f4-4003-baf4-1b55ad6edd54"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "inputs = keras.Input(shape=(128, 128,3))\n",
    "x = layers.Flatten()(inputs)\n",
    "x = layers.Dense(32, activation=\"relu\")(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "outputs = layers.Dense(3, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_437haxhtjFa"
   },
   "source": [
    "**8. Model Compilation and Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DixxjVJelrcg",
    "outputId": "72e41bd4-284b-47c2-c6fe-e61a853b30f6"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "adam = Adam(learning_rate=0.0001)\n",
    "# We are going to use accuracy metrics and cross entropy loss as performance parameters\n",
    "model.compile(adam, loss='categorical_crossentropy', metrics=['acc'])\n",
    "# Train the model \n",
    "history = model.fit(train_generator, \n",
    "      steps_per_epoch=train_generator.samples/train_generator.batch_size,\n",
    "      epochs=100,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=validation_generator.samples/validation_generator.batch_size,\n",
    "      verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7h2kqqa0tpx9"
   },
   "source": [
    "**9. Model saving**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AMlZWxBhnfjh"
   },
   "outputs": [],
   "source": [
    "model.save('covid_classification.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v64VrifGtsSn"
   },
   "source": [
    "**10. Model loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5kAXwzBoIzp"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "model = models.load_model('covid_classification.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EuGiWfjQtvDq"
   },
   "source": [
    "**11. Model weights saving**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "brmKRhGtoZVA"
   },
   "outputs": [],
   "source": [
    "model.save_weights('covid_classification_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_mHfakptxUF"
   },
   "source": [
    "**12. Model weights loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oc3YG_vioL9a"
   },
   "outputs": [],
   "source": [
    "model.load_weights('covid_classification_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJhgz0AAtzcm"
   },
   "source": [
    "**13. Plotting accuracy and loss graph for training and validation dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKLbBshSocU6"
   },
   "outputs": [],
   "source": [
    "train_acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562
    },
    "id": "BfqOqEpVogvN",
    "outputId": "d927be71-b1c2-4b51-f862-5e562269ce4f"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "epochs = range(len(train_acc)) \n",
    "plt.plot(epochs, train_acc, 'b', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epochs, train_loss, 'b', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LoVhONtnt3sq"
   },
   "source": [
    "**14. Evaluate model performance on test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dxXMTffHoiVy",
    "outputId": "ca3d58aa-22f0-4d2c-d01f-23c5c7e07b93"
   },
   "outputs": [],
   "source": [
    "test_output= model.evaluate(test_generator, steps=test_generator.samples/test_generator.batch_size, verbose=1)\n",
    "print(test_output)\n",
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inzAGua1o1we"
   },
   "source": [
    "References:\n",
    "\n",
    "1. https://pypi.org/project/split-folders/\n",
    "2. https://keras.io/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Image_Classification_using_ANN .ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
